{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlYQh5dOb-7e"
   },
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "\n",
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "all_stopwords_gensim = STOPWORDS\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "#Remove \\n\n",
    "dataset['cleanText'] = np.vectorize(remove_pattern)(dataset['text'], '\\n')\n",
    "#Removing Punctuations and special charachers\n",
    "dataset['cleanText'] = dataset['cleanText'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "#removing short words\n",
    "dataset['cleanText'] = dataset['text'].apply(\n",
    "    lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "#removing stop words\n",
    "j=0\n",
    "for all in dataset['cleanText']:\n",
    "  dataset['cleanText'][j] = remove_stopwords(all)\n",
    "  j = j+1\n",
    "\n",
    "#Tokenizing\n",
    "tokenized_text = dataset['cleanText'].apply(lambda x: x.split())\n",
    "#Text Normalization\n",
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer()\n",
    "tokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "for i in range(len(tokenized_text)):\n",
    "    tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "dataset['cleanText'] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZDv-_eScCbp"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ni6b8z-mcDN9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data=[]\n",
    "for x in dataset['cleanText']:\n",
    "  for all in x.split():\n",
    "    data.append(all)\n",
    "\n",
    "#BOW\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=300, \n",
    "                                 stop_words='english') \n",
    "bow = bow_vectorizer.fit_transform(data)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5zf_5q1cGF7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=300, \n",
    "                                   stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(data) \n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpuSsL-qcL5J"
   },
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            data,\n",
    "            size=300, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "model_w2v.train(data, total_examples= len(tokenized_text), epochs=20)\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary continue\n",
    "            if count != 0:\n",
    "                vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(tokenized_text), 300)) \n",
    "for i in range(len(tokenized_text)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 300)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "wordvec_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chzc04gFcNC1"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm  \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def add_label(twt):\n",
    "  output = []\n",
    "  for i, s in zip(twt.index, twt):\n",
    "    output.append(LabeledSentence(s, [\"tweet_\"+str(i)]))\n",
    "  return output\n",
    "\n",
    "tokenized_tweet = dataset['cleanText'].apply(lambda x: x.split()) # tokenizing \n",
    "labeled_tweets = add_label(dataset['cleanText'])  \n",
    "print(labeled_tweets[:1])\n",
    "\n",
    "model_d2v = gensim.models.Doc2Vec(\n",
    "            dm=1,\n",
    "            dm_mean=1,\n",
    "            size=300, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            alpha=0.1,\n",
    "            seed = 34) \n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples= len(data), epochs=20)\n",
    "\n",
    "doc2vec_arrays = np.zeros((len(tokenized_tweet), 300)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    doc2vec_arrays[i,:] = model_d2v.docvecs[i].reshape((1, 300))\n",
    "    docvec_df = pd.DataFrame(doc2vec_arrays) \n",
    "docvec_df.shape "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pragya Lahoti.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
